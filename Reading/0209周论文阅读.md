
## 推荐

[HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/pdf/2601.12681)
🧠背景
传统的方法进行特征交叉一般是先将长序列聚合（or 压缩？）成比较短的token，然后对这些token进行特征交叉。这样“长序列建模→特征交互”的单向流程，即序列压缩后的标记和异构非序列标记之间的交互通常只发生在后期阶段。在当前的模式下，跨特征推理被推迟到序列压缩之后，导致不同标记类型之间的交互比较浅层。模型可能无法从细粒度的跨域特征中受益。
🐍训练方法
- 生成query token
	- 序列token：通过池化操作处理seq token
	- 非序列token：通过多个前馈神经网络MLP处理non-seq token（ns token）
	- 然后将它们concat起来，通过轻量级的网络产生global token
- query token decoding
	- 设计self-attention/swiGLU模块接收seq token将输出作为下一个block的seq token
	- 设计cross-attention模块：将序列token作为kv,将global token作为Q，输出给mlp-mixer作为global token。
	- 设计mlp-mixer模块：将non-seq token concat进cross-attention的输出然后进入mlp-mixer模块。
✨ TIPS 传统方法 vs 本文 GPU 优化方法对比

| **优化维度**     | **传统方法**                                                      | **本文方法**                                                                                        |
| ------------ | ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **数据传输**     | 将完整长序列特征（含重复 token）从 Host → GPU 传输<br>• 例：10,000 个 token 全量传输 | **GPU Pooling**：仅传输唯一特征 ID（稀疏表示）<br>• 例：10,000 个 token → 仅传 2,500 个唯一 ID（稀疏度 75%）               |
| **GPU 显存占用** | 存储完整序列特征向量：<br>• 10,000 × 128D = 1.28M 元素                     | **压缩 embedding 表** + GPU 动态重建：<br>• 仅存 2,500 × 128D = 0.32M 元素                                  |
| **梯度同步方式**   | 所有参数同步更新：<br>• 每步必须等待全局梯度同步完成                                 | **分层异步更新**：<br>• 密集参数：延迟 1 步更新（$W_k = W_{k-1} + g_{k-1}$）<br>• 稀疏参数：即时更新（$W_k = W_{k-1} + g_k$） |


[LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders](https://arxiv.org/pdf/2505.04421)
🧠背景
传统的针对长序列建模的思路是：
- 使用GSU（从超长序列中通过hard/soft search找到最相关的item构成用户感兴趣的序列）然后使用ESU计算短序列和target item 的attention weight
- 将超长序列训练出一个user embedding
- 通过记忆网络缓存关键信息，减少计算复杂度, 以空间换时间

🐍训练方法
- 输入：全局token（包含用户的性别等ns token和候选商品的token）和用户的常规序列token
	- 全局token的作用：
		- 具有完成的注意力感受野，能够感受全局上下文来影响其他的token
		- 由于attention sink（即attention关注序列开始的token ），使得seq token的开始不被过度关注。
	- 加入相对时间差和绝对时间的编码
- token融合：直接concat会使得token缺乏交互，因此对长的token分组，设计轻量级的transformer对分组token进行融合。
- 模型结构
	- Cross Causal Attention从原始的长序列$\mathbf{H}$中采样出$\mathbf{H_s}$子序列作为部分 Query 序列。具体地，实验发现直接使用最近的$k$个交互作为部分 Query 序列的策略效果最好。然后，再在前面拼接 Global Tokens 的表征就得到最终的 Query 矩阵$\mathbf{O} = [\mathbf{G}; \mathbf{H_s}] \in \mathbb{R}^{(m+k) \times d}$
		具体计算如下：

		$$
		\mathbf{Q} = \mathbf{O} \mathbf{W_Q}, \quad \mathbf{K} = \mathbf{R} \mathbf{W_K}, \quad \mathbf{V} = \mathbf{R} \mathbf{W_V}
		$$

		$$
		\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}} + \mathbf{M} \right) \mathbf{V}
		$$

		其中，$\mathbf{W_Q}, \mathbf{W_K}, \mathbf{W_V} \in \mathbb{R}^{d \times d}$, $\mathbf{R} \in \mathbb{R}^{(m+L) \times d}$, $\mathbf{M}$为因果掩码矩阵，而注意力计算结果会再经过FFN做后续处理。

	- Self Causal Attention(其它层)
		在 Cross Causal Attention 之后，后续层使用 Self Causal Attention 组成。



## RL｜SFT ｜On-Policy

[Reinforcement Learning via Self-Distillation](https://arxiv.org/pdf/2601.20802)



[Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/pdf/2601.21204)
帕累托前沿：帕累托最优解是指在给定资源条件下，无法通过在一个目标上的改进而不劣于其他目标的解。**帕累托前沿是指所有帕累托最优解的集合**。




[Shaping capabilities with token-level data filtering](https://arxiv.org/pdf/2601.21571)