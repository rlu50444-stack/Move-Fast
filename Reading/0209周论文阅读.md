
## 推荐

[HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/pdf/2601.12681)
🧠背景
传统的方法进行特征交叉一般是先将长序列聚合（or 压缩？）成比较短的token，然后对这些token进行特征交叉。这样“长序列建模→特征交互”的单向流程，即序列压缩后的标记和异构非序列标记之间的交互通常只发生在后期阶段。在当前的模式下，跨特征推理被推迟到序列压缩之后，导致不同标记类型之间的交互比较浅层。模型可能无法从细粒度的跨域特征中受益。
🐍训练方法
- 生成query token
	- 序列token：通过池化操作处理seq token
	- 非序列token：通过多个前馈神经网络MLP处理non-seq token（ns token）
	- 然后将它们concat起来，通过轻量级的网络产生global token
- query token decoding
	- 设计self-attention/swiGLU模块接收seq token将输出作为下一个block的seq token
	- 设计cross-attention模块：将序列token作为kv,将global token作为Q，输出给mlp-mixer作为global token。
	- 设计mlp-mixer模块：将non-seq token concat进cross-attention的输出然后进入mlp-mixer模块。
✨ TIPS 传统方法 vs 本文 GPU 优化方法对比

| **优化维度**     | **传统方法**                                                      | **本文方法**                                                                                        |
| ------------ | ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **数据传输**     | 将完整长序列特征（含重复 token）从 Host → GPU 传输<br>• 例：10,000 个 token 全量传输 | **GPU Pooling**：仅传输唯一特征 ID（稀疏表示）<br>• 例：10,000 个 token → 仅传 2,500 个唯一 ID（稀疏度 75%）               |
| **GPU 显存占用** | 存储完整序列特征向量：<br>• 10,000 × 128D = 1.28M 元素                     | **压缩 embedding 表** + GPU 动态重建：<br>• 仅存 2,500 × 128D = 0.32M 元素                                  |
| **梯度同步方式**   | 所有参数同步更新：<br>• 每步必须等待全局梯度同步完成                                 | **分层异步更新**：<br>• 密集参数：延迟 1 步更新（$W_k = W_{k-1} + g_{k-1}$）<br>• 稀疏参数：即时更新（$W_k = W_{k-1} + g_k$） |


[LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders](https://arxiv.org/pdf/2505.04421)
🧠背景
传统的针对长序列建模的思路是：
- 使用GSU（从超长序列中通过hard/soft search找到最相关的item构成用户感兴趣的序列）然后使用ESU计算短序列和target item 的attention weight
- 将超长序列训练出一个user embedding
- 通过记忆网络缓存关键信息，减少计算复杂度, 以空间换时间

🐍训练方法
- 输入：全局token（包含用户的性别等ns token和候选商品的token）和用户的常规序列token
	- 全局token的作用：
		- 具有完成的注意力感受野，能够感受全局上下文来影响其他的token
		- 由于attention sink（即attention关注序列开始的token ），使得seq token的开始不被过度关注。
	- 加入相对时间差和绝对时间的编码
- token融合：直接concat会使得token缺乏交互，因此对长的token分组，设计轻量级的transformer对分组token进行融合。
- 模型结构
	- Cross Causal Attention从原始的长序列$\mathbf{H}$中采样出$\mathbf{H_s}$子序列作为部分 Query 序列。具体地，实验发现直接使用最近的$k$个交互作为部分 Query 序列的策略效果最好。然后，再在前面拼接 Global Tokens 的表征就得到最终的 Query 矩阵$\mathbf{O} = [\mathbf{G}; \mathbf{H_s}] \in \mathbb{R}^{(m+k) \times d}$
		具体计算如下：

		$$
		\mathbf{Q} = \mathbf{O} \mathbf{W_Q}, \quad \mathbf{K} = \mathbf{R} \mathbf{W_K}, \quad \mathbf{V} = \mathbf{R} \mathbf{W_V}
		$$

		$$
		\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Softmax}\left( \frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{d}} + \mathbf{M} \right) \mathbf{V}
		$$

		其中，$\mathbf{W_Q}, \mathbf{W_K}, \mathbf{W_V} \in \mathbb{R}^{d \times d}$, $\mathbf{R} \in \mathbb{R}^{(m+L) \times d}$, $\mathbf{M}$为因果掩码矩阵，而注意力计算结果会再经过FFN做后续处理。

	- Self Causal Attention(其它层)
		在 Cross Causal Attention 之后，后续层使用 Self Causal Attention 组成。


***

## RL｜SFT ｜On-Policy

[Reinforcement Learning via Self-Distillation](https://arxiv.org/pdf/2601.20802)



[Scaling Embeddings Outperforms Scaling Experts in Language Models](https://arxiv.org/pdf/2601.21204)
```
帕累托前沿：帕累托最优解是指在给定资源条件下，无法通过在一个目标上的改进而不劣于其他目标的解。**帕累托前沿是指所有帕累托最优解的集合**。
```
🧠 背景
文章对比Moe scaling和扩展 Embedding 参数（Embedding Scaling），发现embedding scaling具有更优的帕累托前沿。其中moe scaling具有一些缺陷，在分布式并行中可能增加通信的代价。而对于embedding scaling可以通过词表扩展（比如n-gram技术）或者结构性扩展在每一层分配独立的embeding。
🐍训练方法
本文采用了N-gram embedding增强的方法。
	对于序列中的第 $i$ 个 Token $t_i$，其增强后的 Embedding $e_i$ 计算方式如下：
	$$
	e_i = \frac{1}{N} \left( E_0(t_i) + \sum_{n=2}^{N} E_n(\mathcal{H}_n(t_{i-n+1}, \ldots, t_i)) \right)
	$$
	其中：
	- $E_0 \in \mathbb{R}^{V_0 \times D}$ 是基础 Embedding 表。
	- $E_n \in \mathbb{R}^{V_n \times D}$ 是扩展的 N-gram Embedding 表。
	- $N$ 是最大 N-gram 阶数。
	- $\mathcal{H}_n$ 是哈希映射函数。
	哈希函数采用多项式滚动哈希（Polynomial Rolling Hash）：
	
	$$
	\mathcal{H}_n(t_{i-n+1}, \ldots, t_i) = \left( \sum_{j=0}^{n-1} t_{i-j} * V_0^j \right) \% V_n
	$$
	为了增强表达能力并减少哈希冲突，模型将每个 N-gram Embedding 表分解为 $K$ 个具有不同词表大小的子表，并引入线性投影。最终形式如下：

$$
e_i = \frac{1}{(N-1)K + 1} \left( E_0(t_i) + \sum_{n=2}^{N} \sum_{k=1}^{K} W_{n,k} E_{n,k}(\mathcal{H}_{n,k}(t_{i-n+1}, \ldots, t_i)) \right)
$$
	 $E_{n,k}$ 是子表，维度为 $V_{n,k} \times \frac{D}{(N-1)K}$。
	$W_{n,k}$ 是线性投影矩阵。
	 通过调整子表维度，确保参数总量对 $N$ 和 $K$ 不敏感。
✨ 实验结果
- 实验做了添加N-gram的embedding在不同的 模型总数/激活参数量 （倍率）下的效果：在低参数倍率下，Embedding Scaling 不如直接增加专家有效；但在高稀疏度水平（高倍率）下，Embedding Scaling 的收益显著超过 Expert Scaling。因此N-gram Embedding的引入是在Moe出现边际效益下降时候。
- 当 N-gram Embedding 占据过多参数预算时，性能会被 MoE 基线反超。Loss 随 N-gram Embedding 比例呈现 U 型曲线。分配给 N-gram Embedding 的参数预算不应超过模型总参数预算的 **50%**
- n-gram的词表大小不要设置成基础词表大小的整数倍


[Shaping capabilities with token-level data filtering](https://arxiv.org/pdf/2601.21571)
GPT之父力作。
本文探究了如何使用Token-level的过滤在**预训练阶段**从而移除LLM的某些特定的能力，同时保留该特定能力的邻近能力。比如保留生物学任务能力但移除掉医学知识能力。当前的主要做法集中在后训练阶段，利用RL等方法让模型忘记特定的权重。
```
这居然是一个方向，叫做机器遗忘（machine unlearning）。主要用于让AI忘记包含用户隐私，或者标注的错误数据，有害数据等等。
```
相对于token-level的过滤，还有文档层面的过滤。即当文档中出现了某些相关的信息就删掉整个文档然后进行训练。而对于token-level的过滤，本文提出了两种干预模式：
- loss-masking：在forward中不移除相关的字符，而backward时候阻断这些token的传播。
	- 在预测其他保留的token的时候仍然能看到需要被mask的token。保持语义连贯。
- removal：直接将相关的token替换成`<hidden>`token。
	-  彻底阻断了信息流，但破坏了文本的连贯性。
对token的判断可以用一个小的判别模型进行。但是我不太理解，何苦从预训练开始呢。



