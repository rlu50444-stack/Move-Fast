[SELF-DISTILLATION ENABLES CONTINUAL LEARNING](https://arxiv.org/pdf/2601.19897)
💻[code is here]([https://github.com/idanshen/Self-Distillation](https://github.com/idanshen/Self-Distillation))
🧠 背景
SFT和RL的优势和缺陷：
- 直接利用sft容易使得模型在训练集上过拟合，而面对测试集(ood)会效果变差。在持续学习的场景下，SFT 倾向于过度拟合当前任务的数据分布，导致模型参数大幅偏离预训练状态，从而引发**灾难性遗忘**。
- 利用RL又难以定义reward model，在许多现实场景（如开放域问答、复杂推理）中，定义一个高质量的奖励函数极其困难，或者不可用。
🐍 训练方法（SDFT）
1. 先整理训练数据，教师模型会增加thinking processing等额外的信息，对于教师模型的分布为
$$
Q = \pi_{\phi}(\cdot \mid x, c).
$$
2. 学生模型的分布是
$$
Q = \pi_{\phi}(\cdot \mid x).
$$
3. 训练的核心是让减小教师模型和学生模型输出的KL散度。利用模型的自回归特性，KL 散度可以分解为 Token 级别的求和。
	对于第 $t$ 步，给定历史 $y_{<t}$，SDFT 计算为：

$$
\nabla_{\theta} \mathcal{L}
\approx
\sum_{t} \sum_{v \in \mathcal{V}}
\left(
\log
\frac{
\pi_{\theta}(v \mid y_{<t}, x)
}{
\pi_{\phi}(v \mid y_{<t}, x, c)
}
\right)
\nabla_{\theta} \pi_{\theta}(v \mid y_{<t}, x)
$$

🎯 在训练过程中，教师模型 $\pi_{\phi}$ 的参数如何设定是一个关键的设计选择。
- **选项 A：冻结的基础模型。**  
  这会导致性能上限受限，因为教师无法随着学生的进步而改进。
- **选项 B：实时学生副本（$\phi = \theta$）。**  
  这会导致训练极不稳定，正反馈循环可能放大噪声。
- **选项 C（SDFT 采用）：指数移动平均（EMA）。**  
  教师参数 $\phi$ 的更新规则为：
  $$
  \phi \leftarrow \alpha \theta + (1 - \alpha)\phi.
  $$

  EMA 能够平滑高方差的更新，同时确保教师随着学生能力的提升而逐步“进化”，
  从而提供更加准确和稳定的指导。
✍️ 实战演练



[Token-level Collaborative Alignment for LLM-based Generative Recommendation](https://www.arxiv.org/pdf/2601.18457)
💻 [code is here !!!](https://github.com/critical88/TCA4Rec)
🧠 背景
现有基于LLM的推荐系统难以有效融合CF信号，原因在于CF模型在物品级别建模用户偏好，而LLM则以token级别的“下一个token预测”（Next-Token Prediction, NTP）进行优化，这导致了根本性的粒度不匹配。以往的方法通常将CF视为上下文提示或表示偏差，并依赖多阶段训练来缓解行为-语义空间差异，使得CF无法明确调控LLM的生成。
🐍 训练方法
- Collaborative Tokenizer
	1. 在LLM自回归生成token $y_j$ 时，根据当前已生成的token序列 $y_{<j} = \{y_1, \ldots, y_{j-1}\}$，筛选出所有与该前缀一致(当前解码流程推断出第一个token)的候选物品集合 $C(y_{<j})$。形式化为：

		$$
		C(y_{<j}) = \{i \in I \mid j \leq |t_i|, (w_{i,1}, \ldots, w_{i,j-1}) = y_{<j}\}
		$$

		其中 $t_i$ 是物品 $i$ 的token序列，$w_{i,k}$ 是 $t_i$ 的第 $k$ 个token。
	2. 对候选物品的原始（sasrac生成的user embedding和item embedding然后计算点乘积）CF logits进行softmax归一化，得到物品级别的概率分布 $\pi_{u,i}^{(j)}$：
			$$
			\pi_{u,i}^{(j)}(y_{<j}) = \frac{\exp(z_{u,i})}{\sum_{k \in C(y_{<j})} \exp(z_{u,k})}
			$$
	3. 将共享同一"下一个token"的所有候选物品的概率进行聚合，形成token级别的概率分布 $p_u(v|y_{<j})$：

		$$
		p_u(v|y_{<j}) = \sum_{i \in C(y_{<j})} \pi_{u,i}^{(j)}(y_{<j}) \cdot \mathbb{1}_{w_{i,j}=v}, \quad v \in V
		$$

		其中 $v$ 是LLM词汇表中的候选token，$\mathbb{1}_{w_{i,j}=v}$ 是指示函数。

- Soft Label Alignment

	1. 将CF导出的token级分布 $p_u(v|y_{<j})$ 与原始的独热（one-hot）监督标签 $\mathbf{1}_{v=y_j}$ 结合，构建软标签 $\tilde{y}_j(v)$：
	
			$$
			\tilde{y}_j(v) = (1 - \alpha)\mathbf{1}_{v=y_j} + \alpha p_u(v|y_{<j}), \quad v \in V
			$$
	
		其中 $\alpha \in [0, 1]$ 是平衡两种监督信号的超参数。
	
	2. 利用这些软标签优化一个软NTP目标，即软交叉熵损失 $\mathcal{L}_{\text{soft-NTP}}$：
	
			$$
			\mathcal{L}_{\text{soft-NTP}} = -\sum_{j=1}^{|t|} \log \sum_{v \in V} \tilde{y}_j(v) \cdot P(v|x_u, y_{<j}; \theta)
			$$
	
		其中 $P(v|x_u, y_{<j}; \theta)$ 是LLM预测下一个token $v$ 的概率。当 $\alpha = 0$ 时，该损失退化为标准的NTP损失；当 $\alpha = 1$ 时，则完全遵循CF分布。

这一tokenizer的方法可以直接用于许多生成式推荐的流程中，比如tiger等。

✍️实战演练



[HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction](https://arxiv.org/pdf/2601.12681)




[Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/pdf/2508.10312)


